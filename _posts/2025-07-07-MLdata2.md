---
layout: post
title: Learning Patterns in Chinese Herbs — A Machine Learning Approach
data_story: true
description: Using machine learning to uncover patterns between herb properties such as taste, nature, and part used. Built from real SymMap data and visualized interactively with Altair.
date: 2025-07-10 10:00:00
image: '/images/24.jpg'
tags: [Machine Learning, TCM, Data Visualization]
tags_color: '#2090df'
---

### Introduction: From Tradition to Prediction

In traditional Chinese medicine (TCM), every herb is described through sensory and energetic dimensions — its **nature (性)**, **taste (味)**, and **part used (部位)**.  
For centuries, physicians have used these attributes to decide how a formula should balance the body’s internal heat and cold, or how to direct treatment toward specific organs.

But can we *predict* these ancient classifications using modern data?

This project applies **machine learning** to real TCM data from  
[**SymMap v1.0 — The Symptom–Herb Mapping Database**](https://www.symmap.org/),  
developed by Peking University and collaborators.  
Using records for **499 herbs**, we trained a model to classify whether a herb is *cold, cool, neutral, warm,* or *hot* — based on its **part used**, **meridian association**, and **flavor**.

---

### Data and Features

Each herb record contains several descriptive features:

- **Nature (性):** how it affects body temperature (e.g., cold, warm, hot)  
- **Taste (味):** main flavor (e.g., bitter, sweet, pungent)  
- **Part Used (部位):** which part of the plant is used (e.g., root, leaf, flower)  
- **Meridians (归经):** organs or energy channels the herb targets  
- **Category (药类):** functional grouping such as tonic, diaphoretic, or detoxifying  

We treated **Nature** as the prediction target.  
All other categorical data were transformed into **one-hot** or **multi-hot vectors** using *pandas* and *scikit-learn*.  
Then, two models were trained for comparison:

1. **Logistic Regression** — interpretable and efficient baseline  
2. **Random Forest** — ensemble method that captures nonlinear relationships  

Model performance was evaluated with **accuracy**, **macro-F1 score**, and a **confusion matrix**.

---

### Visualization: Seeing the Predictions

The interactive chart below (built with **Altair** and exported as `/images/data2.json`) shows the **confusion matrix** of the Random Forest model predicting herb nature.

{% raw %}
<div id="herb-ml" style="max-width:800px;margin:2rem auto;"></div>

<script src="https://cdn.jsdelivr.net/npm/vega@5"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-lite@5"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-embed@6"></script>

<script>
  vegaEmbed("#herb-ml", "/images/data2.json", {actions:false});
</script>
{% endraw %}

**How to read it**
- **Confusion Matrices (RF):**
  - *Nature:* `cold` is predicted relatively better; `cool/hot` are often misclassified as `cold/warm/neutral`.
  - *Taste:* the model over-predicts `bitter`; `salty/sour` are rarely hit (very few samples).
- **Distribution bars:** make the imbalance obvious at a glance.
- **Cluster composition charts:** some clusters show coherent “bitter/cold” groupings that echo classic doctrine.


---

### What we learned from the mistakes
1. **Class imbalance** is the main bottleneck  
   Minority classes (*hot, cool, salty, sour*) are too sparse → the model never learns them well.  
   **Fixes:** class weights (`class_weight='balanced'`), SMOTE, stratified splits, or (for exploration) merging ultrarare labels.

2. **Features are not rich enough**  
   With only part/meridians/category, it’s hard to separate close labels (e.g., *cold* vs *cool*).  
   **Fixes:** add **phytochemical features** (flavonoids, essential oils, alkaloids), **classical text tags**, **pairing co-occurrence** (formula context).

3. **Macro-F1 matters**  
   Macro-F1 treats each class equally and exposes how minority classes are ignored.

---

### Quick nearest-neighbor peek
We also did a simple similarity lookup (cosine on feature vectors). An example top-5 set:
('白果', 'neutral', 'bitter', 0.7303)
('化橘红', 'neutral', 'bitter', 0.7303)
('橘叶', 'neutral', 'bitter', 0.7303)
('款冬花', 'cool', 'bitter', 0.7303)
('天仙子', 'warm', 'bitter', 0.7217)

A readable cluster emerges around **bitter + neutral/cool/warm**, which is consistent with traditional grouping by taste.

---

### What to try next
- **Data**
  - Enrich minority classes (*hot, cool, salty, sour*) where possible.  
  - Add **chemical family** flags (e.g., volatile oils → often *pungent/warm*; flavonoids → often *bitter/cool*).  
  - Build a **co-occurrence graph** from formulas to capture pairing patterns.

- **Modeling**
  - Use **class weights** or **focal loss** for imbalance.  
  - Try **XGBoost/LightGBM** with calibration.  
  - Apply **stratified CV** for stable estimates.

- **Visualization**
  - Show **per-class support** and confidence cues.  
  - Add an **error browser** (click off-diagonal cells to list misclassified samples).

> This isn’t “the model failed”—it’s “the problem setup and data need better alignment.”  
> That’s the real value of turning traditional knowledge into a data-science exercise.

---

### Conclusion
The patterns are there; to make them visible, we need fairer data and richer features.  
The interactive JSON (`/images/data2.json`) acts like a **magnifying glass**: it helps us look honestly at model limits and design a smarter next step.

{: .note }
This work is part of my bilingual TCM education project—using real data and interactive storytelling so young learners can appreciate traditional knowledge *and* scientific method together.a education site — blending observation, coding, and cultural learning into one living story.
